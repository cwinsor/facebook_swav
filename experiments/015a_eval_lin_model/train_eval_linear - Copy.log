

INFO - 12/15/22 10:57:33 - 0:00:00 - ============ Initialized logger ============
INFO - 12/15/22 10:57:33 - 0:00:00 - arch: resnet50
                                     batch_size: 64
                                     data_path: C:\Users\chris\Downloads\ILSVRC\Data\CLS-LOC
                                     decay_epochs: [60, 80]
                                     dist_url: env://
                                     dump_checkpoints: D:\code_cluster\me_swav\facebook_swav\experiments\015a_eval_lin_model\checkpoints
                                     dump_path: D:\code_cluster\me_swav\facebook_swav\experiments\015a_eval_lin_model
                                     epochs: 100
                                     final_lr: 0
                                     gamma: 0.1
                                     global_pooling: True
                                     gpu_to_work_on: 0
                                     is_slurm_job: False
                                     local_rank: 0
                                     lr: 0.3
                                     nesterov: False
                                     pretrained: D:\code_cluster\me_swav\pretrained\swav_800ep_pretrain.pth.tar
                                     rank: 0
                                     scheduler_type: cosine
                                     seed: 31
                                     use_bn: False
                                     wd: 1e-06
                                     workers: 10
                                     world_size: -1
INFO - 12/15/22 10:57:33 - 0:00:00 - The experiment will be stored in D:\code_cluster\me_swav\facebook_swav\experiments\015a_eval_lin_model
                                     

INFO - 12/15/22 10:57:33 - 0:00:00 - 0  _CudaDeviceProperties(name='NVIDIA GeForce RTX 3060', major=8, minor=6, total_memory=12287MB, multi_processor_count=28)
INFO - 12/15/22 10:57:33 - 0:00:00 - 1  _CudaDeviceProperties(name='NVIDIA GeForce RTX 3060', major=8, minor=6, total_memory=12287MB, multi_processor_count=28)
INFO - 12/15/22 10:57:33 - 0:00:00 - build training dataset (start)
INFO - 12/15/22 10:57:41 - 0:00:09 - build training dataset (end)
INFO - 12/15/22 10:57:41 - 0:00:09 - build validation dataset (start)
INFO - 12/15/22 10:57:42 - 0:00:09 - build validation dataset (end)
INFO - 12/15/22 10:57:42 - 0:00:09 - Building data done
INFO - 12/15/22 10:57:43 - 0:00:10 - Load pretrained model with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['module.projection_head.0.weight', 'module.projection_head.0.bias', 'module.projection_head.1.weight', 'module.projection_head.1.bias', 'module.projection_head.1.running_mean', 'module.projection_head.1.running_var', 'module.projection_head.1.num_batches_tracked', 'module.projection_head.3.weight', 'module.projection_head.3.bias', 'module.prototypes.weight'])
INFO - 12/15/22 10:57:43 - 0:00:10 - ============ Starting epoch 0 ... ============
INFO - 12/15/22 10:58:07 - 0:00:35 - Model's state_dict:
INFO - 12/15/22 10:58:07 - 0:00:35 - module.conv1.weight	torch.Size([64, 3, 7, 7])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.bn1.weight	torch.Size([64])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.bn1.bias	torch.Size([64])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.bn1.running_mean	torch.Size([64])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.bn1.running_var	torch.Size([64])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.bn1.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.0.conv1.weight	torch.Size([64, 64, 1, 1])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.0.bn1.weight	torch.Size([64])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.0.bn1.bias	torch.Size([64])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.0.bn1.running_mean	torch.Size([64])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.0.bn1.running_var	torch.Size([64])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.0.bn1.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.0.conv2.weight	torch.Size([64, 64, 3, 3])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.0.bn2.weight	torch.Size([64])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.0.bn2.bias	torch.Size([64])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.0.bn2.running_mean	torch.Size([64])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.0.bn2.running_var	torch.Size([64])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.0.bn2.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.0.conv3.weight	torch.Size([256, 64, 1, 1])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.0.bn3.weight	torch.Size([256])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.0.bn3.bias	torch.Size([256])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.0.bn3.running_mean	torch.Size([256])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.0.bn3.running_var	torch.Size([256])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.0.bn3.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.0.downsample.0.weight	torch.Size([256, 64, 1, 1])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.0.downsample.1.weight	torch.Size([256])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.0.downsample.1.bias	torch.Size([256])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.0.downsample.1.running_mean	torch.Size([256])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.0.downsample.1.running_var	torch.Size([256])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.0.downsample.1.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.1.conv1.weight	torch.Size([64, 256, 1, 1])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.1.bn1.weight	torch.Size([64])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.1.bn1.bias	torch.Size([64])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.1.bn1.running_mean	torch.Size([64])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.1.bn1.running_var	torch.Size([64])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.1.bn1.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.1.conv2.weight	torch.Size([64, 64, 3, 3])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.1.bn2.weight	torch.Size([64])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.1.bn2.bias	torch.Size([64])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.1.bn2.running_mean	torch.Size([64])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.1.bn2.running_var	torch.Size([64])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.1.bn2.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.1.conv3.weight	torch.Size([256, 64, 1, 1])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.1.bn3.weight	torch.Size([256])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.1.bn3.bias	torch.Size([256])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.1.bn3.running_mean	torch.Size([256])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.1.bn3.running_var	torch.Size([256])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.1.bn3.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.2.conv1.weight	torch.Size([64, 256, 1, 1])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.2.bn1.weight	torch.Size([64])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.2.bn1.bias	torch.Size([64])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.2.bn1.running_mean	torch.Size([64])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.2.bn1.running_var	torch.Size([64])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.2.bn1.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.2.conv2.weight	torch.Size([64, 64, 3, 3])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.2.bn2.weight	torch.Size([64])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.2.bn2.bias	torch.Size([64])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.2.bn2.running_mean	torch.Size([64])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.2.bn2.running_var	torch.Size([64])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.2.bn2.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.2.conv3.weight	torch.Size([256, 64, 1, 1])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.2.bn3.weight	torch.Size([256])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.2.bn3.bias	torch.Size([256])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.2.bn3.running_mean	torch.Size([256])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.2.bn3.running_var	torch.Size([256])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer1.2.bn3.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer2.0.conv1.weight	torch.Size([128, 256, 1, 1])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer2.0.bn1.weight	torch.Size([128])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer2.0.bn1.bias	torch.Size([128])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer2.0.bn1.running_mean	torch.Size([128])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer2.0.bn1.running_var	torch.Size([128])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer2.0.bn1.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer2.0.conv2.weight	torch.Size([128, 128, 3, 3])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer2.0.bn2.weight	torch.Size([128])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer2.0.bn2.bias	torch.Size([128])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer2.0.bn2.running_mean	torch.Size([128])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer2.0.bn2.running_var	torch.Size([128])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer2.0.bn2.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer2.0.conv3.weight	torch.Size([512, 128, 1, 1])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer2.0.bn3.weight	torch.Size([512])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer2.0.bn3.bias	torch.Size([512])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer2.0.bn3.running_mean	torch.Size([512])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer2.0.bn3.running_var	torch.Size([512])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer2.0.bn3.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer2.0.downsample.0.weight	torch.Size([512, 256, 1, 1])
INFO - 12/15/22 10:58:07 - 0:00:35 - module.layer2.0.downsample.1.weight	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.0.downsample.1.bias	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.0.downsample.1.running_mean	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.0.downsample.1.running_var	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.0.downsample.1.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.1.conv1.weight	torch.Size([128, 512, 1, 1])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.1.bn1.weight	torch.Size([128])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.1.bn1.bias	torch.Size([128])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.1.bn1.running_mean	torch.Size([128])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.1.bn1.running_var	torch.Size([128])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.1.bn1.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.1.conv2.weight	torch.Size([128, 128, 3, 3])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.1.bn2.weight	torch.Size([128])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.1.bn2.bias	torch.Size([128])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.1.bn2.running_mean	torch.Size([128])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.1.bn2.running_var	torch.Size([128])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.1.bn2.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.1.conv3.weight	torch.Size([512, 128, 1, 1])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.1.bn3.weight	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.1.bn3.bias	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.1.bn3.running_mean	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.1.bn3.running_var	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.1.bn3.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.2.conv1.weight	torch.Size([128, 512, 1, 1])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.2.bn1.weight	torch.Size([128])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.2.bn1.bias	torch.Size([128])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.2.bn1.running_mean	torch.Size([128])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.2.bn1.running_var	torch.Size([128])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.2.bn1.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.2.conv2.weight	torch.Size([128, 128, 3, 3])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.2.bn2.weight	torch.Size([128])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.2.bn2.bias	torch.Size([128])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.2.bn2.running_mean	torch.Size([128])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.2.bn2.running_var	torch.Size([128])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.2.bn2.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.2.conv3.weight	torch.Size([512, 128, 1, 1])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.2.bn3.weight	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.2.bn3.bias	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.2.bn3.running_mean	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.2.bn3.running_var	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.2.bn3.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.3.conv1.weight	torch.Size([128, 512, 1, 1])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.3.bn1.weight	torch.Size([128])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.3.bn1.bias	torch.Size([128])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.3.bn1.running_mean	torch.Size([128])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.3.bn1.running_var	torch.Size([128])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.3.bn1.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.3.conv2.weight	torch.Size([128, 128, 3, 3])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.3.bn2.weight	torch.Size([128])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.3.bn2.bias	torch.Size([128])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.3.bn2.running_mean	torch.Size([128])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.3.bn2.running_var	torch.Size([128])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.3.bn2.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.3.conv3.weight	torch.Size([512, 128, 1, 1])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.3.bn3.weight	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.3.bn3.bias	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.3.bn3.running_mean	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.3.bn3.running_var	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer2.3.bn3.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.0.conv1.weight	torch.Size([256, 512, 1, 1])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.0.bn1.weight	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.0.bn1.bias	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.0.bn1.running_mean	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.0.bn1.running_var	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.0.bn1.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.0.conv2.weight	torch.Size([256, 256, 3, 3])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.0.bn2.weight	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.0.bn2.bias	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.0.bn2.running_mean	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.0.bn2.running_var	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.0.bn2.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.0.conv3.weight	torch.Size([1024, 256, 1, 1])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.0.bn3.weight	torch.Size([1024])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.0.bn3.bias	torch.Size([1024])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.0.bn3.running_mean	torch.Size([1024])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.0.bn3.running_var	torch.Size([1024])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.0.bn3.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.0.downsample.0.weight	torch.Size([1024, 512, 1, 1])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.0.downsample.1.weight	torch.Size([1024])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.0.downsample.1.bias	torch.Size([1024])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.0.downsample.1.running_mean	torch.Size([1024])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.0.downsample.1.running_var	torch.Size([1024])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.0.downsample.1.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.1.conv1.weight	torch.Size([256, 1024, 1, 1])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.1.bn1.weight	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.1.bn1.bias	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.1.bn1.running_mean	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.1.bn1.running_var	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.1.bn1.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.1.conv2.weight	torch.Size([256, 256, 3, 3])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.1.bn2.weight	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.1.bn2.bias	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.1.bn2.running_mean	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.1.bn2.running_var	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.1.bn2.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.1.conv3.weight	torch.Size([1024, 256, 1, 1])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.1.bn3.weight	torch.Size([1024])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.1.bn3.bias	torch.Size([1024])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.1.bn3.running_mean	torch.Size([1024])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.1.bn3.running_var	torch.Size([1024])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.1.bn3.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.2.conv1.weight	torch.Size([256, 1024, 1, 1])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.2.bn1.weight	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.2.bn1.bias	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.2.bn1.running_mean	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.2.bn1.running_var	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.2.bn1.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.2.conv2.weight	torch.Size([256, 256, 3, 3])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.2.bn2.weight	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.2.bn2.bias	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.2.bn2.running_mean	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.2.bn2.running_var	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.2.bn2.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.2.conv3.weight	torch.Size([1024, 256, 1, 1])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.2.bn3.weight	torch.Size([1024])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.2.bn3.bias	torch.Size([1024])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.2.bn3.running_mean	torch.Size([1024])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.2.bn3.running_var	torch.Size([1024])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.2.bn3.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.3.conv1.weight	torch.Size([256, 1024, 1, 1])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.3.bn1.weight	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.3.bn1.bias	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.3.bn1.running_mean	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.3.bn1.running_var	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.3.bn1.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.3.conv2.weight	torch.Size([256, 256, 3, 3])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.3.bn2.weight	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.3.bn2.bias	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.3.bn2.running_mean	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.3.bn2.running_var	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.3.bn2.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.3.conv3.weight	torch.Size([1024, 256, 1, 1])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.3.bn3.weight	torch.Size([1024])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.3.bn3.bias	torch.Size([1024])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.3.bn3.running_mean	torch.Size([1024])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.3.bn3.running_var	torch.Size([1024])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.3.bn3.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.4.conv1.weight	torch.Size([256, 1024, 1, 1])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.4.bn1.weight	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.4.bn1.bias	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.4.bn1.running_mean	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.4.bn1.running_var	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.4.bn1.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.4.conv2.weight	torch.Size([256, 256, 3, 3])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.4.bn2.weight	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.4.bn2.bias	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.4.bn2.running_mean	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.4.bn2.running_var	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.4.bn2.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.4.conv3.weight	torch.Size([1024, 256, 1, 1])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.4.bn3.weight	torch.Size([1024])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.4.bn3.bias	torch.Size([1024])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.4.bn3.running_mean	torch.Size([1024])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.4.bn3.running_var	torch.Size([1024])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.4.bn3.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.5.conv1.weight	torch.Size([256, 1024, 1, 1])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.5.bn1.weight	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.5.bn1.bias	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.5.bn1.running_mean	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.5.bn1.running_var	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.5.bn1.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.5.conv2.weight	torch.Size([256, 256, 3, 3])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.5.bn2.weight	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.5.bn2.bias	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.5.bn2.running_mean	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.5.bn2.running_var	torch.Size([256])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.5.bn2.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.5.conv3.weight	torch.Size([1024, 256, 1, 1])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.5.bn3.weight	torch.Size([1024])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.5.bn3.bias	torch.Size([1024])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.5.bn3.running_mean	torch.Size([1024])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.5.bn3.running_var	torch.Size([1024])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer3.5.bn3.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.0.conv1.weight	torch.Size([512, 1024, 1, 1])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.0.bn1.weight	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.0.bn1.bias	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.0.bn1.running_mean	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.0.bn1.running_var	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.0.bn1.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.0.conv2.weight	torch.Size([512, 512, 3, 3])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.0.bn2.weight	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.0.bn2.bias	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.0.bn2.running_mean	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.0.bn2.running_var	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.0.bn2.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.0.conv3.weight	torch.Size([2048, 512, 1, 1])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.0.bn3.weight	torch.Size([2048])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.0.bn3.bias	torch.Size([2048])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.0.bn3.running_mean	torch.Size([2048])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.0.bn3.running_var	torch.Size([2048])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.0.bn3.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.0.downsample.0.weight	torch.Size([2048, 1024, 1, 1])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.0.downsample.1.weight	torch.Size([2048])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.0.downsample.1.bias	torch.Size([2048])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.0.downsample.1.running_mean	torch.Size([2048])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.0.downsample.1.running_var	torch.Size([2048])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.0.downsample.1.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.1.conv1.weight	torch.Size([512, 2048, 1, 1])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.1.bn1.weight	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.1.bn1.bias	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.1.bn1.running_mean	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.1.bn1.running_var	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.1.bn1.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.1.conv2.weight	torch.Size([512, 512, 3, 3])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.1.bn2.weight	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.1.bn2.bias	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.1.bn2.running_mean	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.1.bn2.running_var	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.1.bn2.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.1.conv3.weight	torch.Size([2048, 512, 1, 1])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.1.bn3.weight	torch.Size([2048])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.1.bn3.bias	torch.Size([2048])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.1.bn3.running_mean	torch.Size([2048])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.1.bn3.running_var	torch.Size([2048])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.1.bn3.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.2.conv1.weight	torch.Size([512, 2048, 1, 1])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.2.bn1.weight	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.2.bn1.bias	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.2.bn1.running_mean	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.2.bn1.running_var	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.2.bn1.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.2.conv2.weight	torch.Size([512, 512, 3, 3])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.2.bn2.weight	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.2.bn2.bias	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.2.bn2.running_mean	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.2.bn2.running_var	torch.Size([512])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.2.bn2.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.2.conv3.weight	torch.Size([2048, 512, 1, 1])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.2.bn3.weight	torch.Size([2048])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.2.bn3.bias	torch.Size([2048])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.2.bn3.running_mean	torch.Size([2048])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.2.bn3.running_var	torch.Size([2048])
INFO - 12/15/22 10:58:08 - 0:00:35 - module.layer4.2.bn3.num_batches_tracked	torch.Size([])
INFO - 12/15/22 10:58:08 - 0:00:35 - info.model:
INFO - 12/15/22 10:58:08 - 0:00:35 - DataParallel(
                                       (module): ResNet(
                                         (padding): ConstantPad2d(padding=(1, 1, 1, 1), value=0.0)
                                         (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(2, 2), bias=False)
                                         (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                         (relu): ReLU(inplace=True)
                                         (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
                                         (layer1): Sequential(
                                           (0): Bottleneck(
                                             (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                             (downsample): Sequential(
                                               (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                               (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             )
                                           )
                                           (1): Bottleneck(
                                             (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                           )
                                           (2): Bottleneck(
                                             (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                           )
                                         )
                                         (layer2): Sequential(
                                           (0): Bottleneck(
                                             (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                             (downsample): Sequential(
                                               (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
                                               (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             )
                                           )
                                           (1): Bottleneck(
                                             (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                           )
                                           (2): Bottleneck(
                                             (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                           )
                                           (3): Bottleneck(
                                             (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                           )
                                         )
                                         (layer3): Sequential(
                                           (0): Bottleneck(
                                             (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                             (downsample): Sequential(
                                               (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
                                               (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             )
                                           )
                                           (1): Bottleneck(
                                             (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                           )
                                           (2): Bottleneck(
                                             (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                           )
                                           (3): Bottleneck(
                                             (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                           )
                                           (4): Bottleneck(
                                             (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                           )
                                           (5): Bottleneck(
                                             (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                           )
                                         )
                                         (layer4): Sequential(
                                           (0): Bottleneck(
                                             (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                             (downsample): Sequential(
                                               (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
                                               (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             )
                                           )
                                           (1): Bottleneck(
                                             (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                           )
                                           (2): Bottleneck(
                                             (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                                             (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                             (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                                             (relu): ReLU(inplace=True)
                                           )
                                         )
                                         (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
                                       )
                                     )
INFO - 12/15/22 10:58:11 - 0:00:39 - Epoch[0] - Iter: [0/20019]	Time 28.462 (28.462)	Data 24.341 (24.341)	Loss 6.9162 (6.9162)	Prec 0.000 (0.000)	LR 0.3
INFO - 12/15/22 10:58:16 - 0:00:43 - Epoch[0] - Iter: [50/20019]	Time 0.093 (0.650)	Data 0.000 (0.478)	Loss 6.4493 (6.8031)	Prec 6.250 (2.543)	LR 0.3
INFO - 12/15/22 10:58:21 - 0:00:48 - Epoch[0] - Iter: [100/20019]	Time 0.093 (0.374)	Data 0.000 (0.241)	Loss 5.6339 (6.4539)	Prec 20.312 (6.173)	LR 0.3
INFO - 12/15/22 10:58:25 - 0:00:53 - Epoch[0] - Iter: [150/20019]	Time 0.093 (0.281)	Data 0.000 (0.161)	Loss 4.8819 (6.1156)	Prec 26.562 (9.261)	LR 0.3
INFO - 12/15/22 10:58:30 - 0:00:57 - Epoch[0] - Iter: [200/20019]	Time 0.092 (0.235)	Data 0.000 (0.121)	Loss 4.7676 (5.8090)	Prec 20.312 (12.158)	LR 0.3
INFO - 12/15/22 10:58:35 - 0:01:02 - Epoch[0] - Iter: [250/20019]	Time 0.093 (0.207)	Data 0.000 (0.097)	Loss 4.1126 (5.5299)	Prec 28.125 (14.909)	LR 0.3
INFO - 12/15/22 10:58:40 - 0:01:07 - Epoch[0] - Iter: [300/20019]	Time 0.093 (0.188)	Data 0.000 (0.081)	Loss 4.0759 (5.2811)	Prec 32.812 (17.302)	LR 0.3
INFO - 12/15/22 10:58:44 - 0:01:11 - Epoch[0] - Iter: [350/20019]	Time 0.092 (0.174)	Data 0.000 (0.070)	Loss 3.6167 (5.0645)	Prec 29.688 (19.667)	LR 0.3
INFO - 12/15/22 10:58:49 - 0:01:16 - Epoch[0] - Iter: [400/20019]	Time 0.093 (0.164)	Data 0.000 (0.061)	Loss 3.2717 (4.8664)	Prec 45.312 (21.774)	LR 0.3
INFO - 12/15/22 10:58:54 - 0:01:21 - Epoch[0] - Iter: [450/20019]	Time 0.094 (0.156)	Data 0.000 (0.054)	Loss 3.2688 (4.6905)	Prec 37.500 (23.787)	LR 0.3
INFO - 12/15/22 10:58:58 - 0:01:25 - Epoch[0] - Iter: [500/20019]	Time 0.093 (0.150)	Data 0.000 (0.049)	Loss 2.5444 (4.5337)	Prec 51.562 (25.533)	LR 0.3
INFO - 12/15/22 10:59:03 - 0:01:30 - Epoch[0] - Iter: [550/20019]	Time 0.093 (0.145)	Data 0.000 (0.044)	Loss 2.6958 (4.4020)	Prec 46.875 (27.005)	LR 0.3
INFO - 12/15/22 10:59:08 - 0:01:35 - Epoch[0] - Iter: [600/20019]	Time 0.092 (0.141)	Data 0.000 (0.041)	Loss 3.1438 (4.2776)	Prec 39.062 (28.403)	LR 0.3
INFO - 12/15/22 10:59:12 - 0:01:40 - Epoch[0] - Iter: [650/20019]	Time 0.093 (0.137)	Data 0.000 (0.038)	Loss 2.4970 (4.1686)	Prec 51.562 (29.714)	LR 0.3
INFO - 12/15/22 10:59:17 - 0:01:44 - Epoch[0] - Iter: [700/20019]	Time 0.092 (0.134)	Data 0.000 (0.035)	Loss 2.8147 (4.0676)	Prec 48.438 (30.824)	LR 0.3
INFO - 12/15/22 10:59:22 - 0:01:49 - Epoch[0] - Iter: [750/20019]	Time 0.095 (0.131)	Data 0.000 (0.033)	Loss 2.3889 (3.9763)	Prec 48.438 (31.889)	LR 0.3
INFO - 12/15/22 10:59:26 - 0:01:54 - Epoch[0] - Iter: [800/20019]	Time 0.093 (0.129)	Data 0.000 (0.031)	Loss 2.8674 (3.8925)	Prec 39.062 (32.828)	LR 0.3
INFO - 12/15/22 10:59:31 - 0:01:58 - Epoch[0] - Iter: [850/20019]	Time 0.093 (0.127)	Data 0.000 (0.029)	Loss 2.7196 (3.8198)	Prec 43.750 (33.648)	LR 0.3
INFO - 12/15/22 10:59:36 - 0:02:03 - Epoch[0] - Iter: [900/20019]	Time 0.092 (0.125)	Data 0.000 (0.027)	Loss 2.7172 (3.7496)	Prec 46.875 (34.427)	LR 0.3
INFO - 12/15/22 10:59:40 - 0:02:08 - Epoch[0] - Iter: [950/20019]	Time 0.094 (0.123)	Data 0.000 (0.026)	Loss 2.3441 (3.6841)	Prec 48.438 (35.238)	LR 0.3
INFO - 12/15/22 10:59:45 - 0:02:12 - Epoch[0] - Iter: [1000/20019]	Time 0.094 (0.122)	Data 0.000 (0.025)	Loss 2.4546 (3.6229)	Prec 46.875 (35.895)	LR 0.3
INFO - 12/15/22 10:59:50 - 0:02:17 - Epoch[0] - Iter: [1050/20019]	Time 0.093 (0.121)	Data 0.000 (0.023)	Loss 2.3963 (3.5660)	Prec 53.125 (36.609)	LR 0.3
INFO - 12/15/22 10:59:54 - 0:02:22 - Epoch[0] - Iter: [1100/20019]	Time 0.096 (0.119)	Data 0.000 (0.022)	Loss 2.5513 (3.5109)	Prec 46.875 (37.212)	LR 0.3
INFO - 12/15/22 10:59:59 - 0:02:26 - Epoch[0] - Iter: [1150/20019]	Time 0.093 (0.118)	Data 0.000 (0.021)	Loss 2.4410 (3.4649)	Prec 46.875 (37.750)	LR 0.3
INFO - 12/15/22 11:00:04 - 0:02:31 - Epoch[0] - Iter: [1200/20019]	Time 0.099 (0.117)	Data 0.000 (0.021)	Loss 2.3747 (3.4189)	Prec 43.750 (38.268)	LR 0.3
INFO - 12/15/22 11:00:09 - 0:02:36 - Epoch[0] - Iter: [1250/20019]	Time 0.093 (0.116)	Data 0.000 (0.020)	Loss 2.1022 (3.3760)	Prec 56.250 (38.783)	LR 0.3
INFO - 12/15/22 11:00:13 - 0:02:40 - Epoch[0] - Iter: [1300/20019]	Time 0.093 (0.115)	Data 0.000 (0.019)	Loss 2.0688 (3.3335)	Prec 51.562 (39.289)	LR 0.3
INFO - 12/15/22 11:00:18 - 0:02:45 - Epoch[0] - Iter: [1350/20019]	Time 0.093 (0.115)	Data 0.000 (0.018)	Loss 1.9608 (3.2936)	Prec 56.250 (39.795)	LR 0.3
INFO - 12/15/22 11:00:23 - 0:02:50 - Epoch[0] - Iter: [1400/20019]	Time 0.093 (0.114)	Data 0.000 (0.018)	Loss 2.2488 (3.2564)	Prec 48.438 (40.220)	LR 0.3
INFO - 12/15/22 11:00:27 - 0:02:55 - Epoch[0] - Iter: [1450/20019]	Time 0.094 (0.113)	Data 0.001 (0.017)	Loss 2.4462 (3.2187)	Prec 56.250 (40.692)	LR 0.3
INFO - 12/15/22 11:00:32 - 0:02:59 - Epoch[0] - Iter: [1500/20019]	Time 0.093 (0.113)	Data 0.000 (0.016)	Loss 1.7531 (3.1844)	Prec 67.188 (41.114)	LR 0.3
INFO - 12/15/22 11:00:37 - 0:03:04 - Epoch[0] - Iter: [1550/20019]	Time 0.093 (0.112)	Data 0.000 (0.016)	Loss 2.2067 (3.1531)	Prec 45.312 (41.467)	LR 0.3
INFO - 12/15/22 11:00:41 - 0:03:09 - Epoch[0] - Iter: [1600/20019]	Time 0.093 (0.111)	Data 0.000 (0.015)	Loss 2.5841 (3.1224)	Prec 51.562 (41.859)	LR 0.3
INFO - 12/15/22 11:00:46 - 0:03:13 - Epoch[0] - Iter: [1650/20019]	Time 0.094 (0.111)	Data 0.000 (0.015)	Loss 1.8762 (3.0949)	Prec 57.812 (42.171)	LR 0.3
INFO - 12/15/22 11:00:51 - 0:03:18 - Epoch[0] - Iter: [1700/20019]	Time 0.094 (0.110)	Data 0.000 (0.015)	Loss 2.1623 (3.0673)	Prec 53.125 (42.539)	LR 0.3
INFO - 12/15/22 11:00:55 - 0:03:23 - Epoch[0] - Iter: [1750/20019]	Time 0.093 (0.110)	Data 0.000 (0.014)	Loss 2.1601 (3.0397)	Prec 53.125 (42.897)	LR 0.3
INFO - 12/15/22 11:01:00 - 0:03:28 - Epoch[0] - Iter: [1800/20019]	Time 0.094 (0.110)	Data 0.000 (0.014)	Loss 2.6903 (3.0144)	Prec 40.625 (43.209)	LR 0.3
INFO - 12/15/22 11:01:05 - 0:03:32 - Epoch[0] - Iter: [1850/20019]	Time 0.094 (0.109)	Data 0.000 (0.013)	Loss 2.3126 (2.9884)	Prec 53.125 (43.552)	LR 0.3
INFO - 12/15/22 11:01:10 - 0:03:37 - Epoch[0] - Iter: [1900/20019]	Time 0.094 (0.109)	Data 0.000 (0.013)	Loss 2.4180 (2.9642)	Prec 43.750 (43.840)	LR 0.3
INFO - 12/15/22 11:01:14 - 0:03:42 - Epoch[0] - Iter: [1950/20019]	Time 0.093 (0.108)	Data 0.000 (0.013)	Loss 2.7797 (2.9419)	Prec 40.625 (44.101)	LR 0.3
INFO - 12/15/22 11:01:19 - 0:03:46 - Epoch[0] - Iter: [2000/20019]	Time 0.100 (0.108)	Data 0.000 (0.012)	Loss 2.0175 (2.9207)	Prec 53.125 (44.381)	LR 0.3
INFO - 12/15/22 11:01:24 - 0:03:51 - Epoch[0] - Iter: [2050/20019]	Time 0.093 (0.108)	Data 0.000 (0.012)	Loss 2.1902 (2.8993)	Prec 48.438 (44.638)	LR 0.3
INFO - 12/15/22 11:01:28 - 0:03:56 - Epoch[0] - Iter: [2100/20019]	Time 0.095 (0.107)	Data 0.000 (0.012)	Loss 2.1130 (2.8783)	Prec 50.000 (44.888)	LR 0.3
INFO - 12/15/22 11:01:33 - 0:04:00 - Epoch[0] - Iter: [2150/20019]	Time 0.095 (0.107)	Data 0.000 (0.012)	Loss 1.8266 (2.8584)	Prec 53.125 (45.140)	LR 0.3
INFO - 12/15/22 11:01:38 - 0:04:05 - Epoch[0] - Iter: [2200/20019]	Time 0.093 (0.107)	Data 0.000 (0.011)	Loss 2.3330 (2.8400)	Prec 54.688 (45.394)	LR 0.3
INFO - 12/15/22 11:01:43 - 0:04:10 - Epoch[0] - Iter: [2250/20019]	Time 0.095 (0.106)	Data 0.000 (0.011)	Loss 1.4851 (2.8201)	Prec 70.312 (45.646)	LR 0.3
INFO - 12/15/22 11:01:47 - 0:04:15 - Epoch[0] - Iter: [2300/20019]	Time 0.094 (0.106)	Data 0.000 (0.011)	Loss 2.2273 (2.8023)	Prec 53.125 (45.875)	LR 0.3
INFO - 12/15/22 11:01:52 - 0:04:19 - Epoch[0] - Iter: [2350/20019]	Time 0.093 (0.106)	Data 0.000 (0.011)	Loss 2.0640 (2.7851)	Prec 50.000 (46.109)	LR 0.3
INFO - 12/15/22 11:01:57 - 0:04:24 - Epoch[0] - Iter: [2400/20019]	Time 0.093 (0.106)	Data 0.000 (0.010)	Loss 2.3159 (2.7684)	Prec 45.312 (46.330)	LR 0.3
INFO - 12/15/22 11:02:02 - 0:04:29 - Epoch[0] - Iter: [2450/20019]	Time 0.093 (0.105)	Data 0.000 (0.010)	Loss 2.2085 (2.7536)	Prec 50.000 (46.510)	LR 0.3
INFO - 12/15/22 11:02:06 - 0:04:34 - Epoch[0] - Iter: [2500/20019]	Time 0.093 (0.105)	Data 0.000 (0.010)	Loss 1.9842 (2.7376)	Prec 50.000 (46.708)	LR 0.3
INFO - 12/15/22 11:02:11 - 0:04:38 - Epoch[0] - Iter: [2550/20019]	Time 0.093 (0.105)	Data 0.000 (0.010)	Loss 1.8157 (2.7222)	Prec 51.562 (46.893)	LR 0.3
INFO - 12/15/22 11:02:16 - 0:04:43 - Epoch[0] - Iter: [2600/20019]	Time 0.094 (0.105)	Data 0.000 (0.010)	Loss 1.9470 (2.7071)	Prec 60.938 (47.073)	LR 0.3
INFO - 12/15/22 11:02:20 - 0:04:48 - Epoch[0] - Iter: [2650/20019]	Time 0.093 (0.105)	Data 0.000 (0.009)	Loss 2.6910 (2.6925)	Prec 40.625 (47.258)	LR 0.3
INFO - 12/15/22 11:02:25 - 0:04:52 - Epoch[0] - Iter: [2700/20019]	Time 0.093 (0.104)	Data 0.000 (0.009)	Loss 2.2711 (2.6791)	Prec 54.688 (47.422)	LR 0.3
INFO - 12/15/22 11:02:30 - 0:04:57 - Epoch[0] - Iter: [2750/20019]	Time 0.094 (0.104)	Data 0.000 (0.009)	Loss 1.5767 (2.6661)	Prec 62.500 (47.592)	LR 0.3
INFO - 12/15/22 11:02:35 - 0:05:02 - Epoch[0] - Iter: [2800/20019]	Time 0.094 (0.104)	Data 0.000 (0.009)	Loss 2.5274 (2.6521)	Prec 51.562 (47.805)	LR 0.3
INFO - 12/15/22 11:02:39 - 0:05:07 - Epoch[0] - Iter: [2850/20019]	Time 0.093 (0.104)	Data 0.000 (0.009)	Loss 1.6351 (2.6408)	Prec 67.188 (47.944)	LR 0.3
INFO - 12/15/22 11:02:44 - 0:05:11 - Epoch[0] - Iter: [2900/20019]	Time 0.094 (0.104)	Data 0.000 (0.009)	Loss 1.6519 (2.6286)	Prec 67.188 (48.093)	LR 0.3
INFO - 12/15/22 11:02:49 - 0:05:16 - Epoch[0] - Iter: [2950/20019]	Time 0.096 (0.104)	Data 0.000 (0.008)	Loss 2.1128 (2.6172)	Prec 51.562 (48.243)	LR 0.3
INFO - 12/15/22 11:02:53 - 0:05:21 - Epoch[0] - Iter: [3000/20019]	Time 0.093 (0.103)	Data 0.000 (0.008)	Loss 1.8098 (2.6052)	Prec 60.938 (48.400)	LR 0.3
INFO - 12/15/22 11:02:58 - 0:05:25 - Epoch[0] - Iter: [3050/20019]	Time 0.094 (0.103)	Data 0.000 (0.008)	Loss 1.7295 (2.5945)	Prec 62.500 (48.529)	LR 0.3
INFO - 12/15/22 11:03:03 - 0:05:30 - Epoch[0] - Iter: [3100/20019]	Time 0.094 (0.103)	Data 0.000 (0.008)	Loss 1.9947 (2.5828)	Prec 51.562 (48.675)	LR 0.3
INFO - 12/15/22 11:03:08 - 0:05:35 - Epoch[0] - Iter: [3150/20019]	Time 0.093 (0.103)	Data 0.000 (0.008)	Loss 1.7173 (2.5718)	Prec 59.375 (48.827)	LR 0.3
INFO - 12/15/22 11:03:12 - 0:05:40 - Epoch[0] - Iter: [3200/20019]	Time 0.093 (0.103)	Data 0.000 (0.008)	Loss 1.9186 (2.5606)	Prec 59.375 (48.976)	LR 0.3
INFO - 12/15/22 11:03:17 - 0:05:44 - Epoch[0] - Iter: [3250/20019]	Time 0.094 (0.103)	Data 0.000 (0.008)	Loss 1.7963 (2.5502)	Prec 57.812 (49.110)	LR 0.3
INFO - 12/15/22 11:03:22 - 0:05:49 - Epoch[0] - Iter: [3300/20019]	Time 0.096 (0.103)	Data 0.000 (0.008)	Loss 2.0960 (2.5407)	Prec 54.688 (49.228)	LR 0.3
INFO - 12/15/22 11:03:26 - 0:05:54 - Epoch[0] - Iter: [3350/20019]	Time 0.096 (0.102)	Data 0.000 (0.008)	Loss 2.2312 (2.5315)	Prec 50.000 (49.359)	LR 0.3
INFO - 12/15/22 11:03:31 - 0:05:58 - Epoch[0] - Iter: [3400/20019]	Time 0.094 (0.102)	Data 0.000 (0.007)	Loss 2.1357 (2.5221)	Prec 54.688 (49.468)	LR 0.3
INFO - 12/15/22 11:03:36 - 0:06:03 - Epoch[0] - Iter: [3450/20019]	Time 0.093 (0.102)	Data 0.000 (0.007)	Loss 1.8176 (2.5122)	Prec 59.375 (49.612)	LR 0.3
INFO - 12/15/22 11:03:41 - 0:06:08 - Epoch[0] - Iter: [3500/20019]	Time 0.094 (0.102)	Data 0.000 (0.007)	Loss 1.7930 (2.5033)	Prec 60.938 (49.733)	LR 0.3
INFO - 12/15/22 11:03:45 - 0:06:13 - Epoch[0] - Iter: [3550/20019]	Time 0.094 (0.102)	Data 0.000 (0.007)	Loss 1.7073 (2.4938)	Prec 64.062 (49.870)	LR 0.3
INFO - 12/15/22 11:03:50 - 0:06:17 - Epoch[0] - Iter: [3600/20019]	Time 0.098 (0.102)	Data 0.000 (0.007)	Loss 2.2157 (2.4850)	Prec 46.875 (49.993)	LR 0.3
INFO - 12/15/22 11:03:55 - 0:06:22 - Epoch[0] - Iter: [3650/20019]	Time 0.095 (0.102)	Data 0.000 (0.007)	Loss 2.4964 (2.4754)	Prec 45.312 (50.117)	LR 0.3
INFO - 12/15/22 11:04:00 - 0:06:27 - Epoch[0] - Iter: [3700/20019]	Time 0.093 (0.102)	Data 0.000 (0.007)	Loss 2.3995 (2.4669)	Prec 46.875 (50.215)	LR 0.3
INFO - 12/15/22 11:04:04 - 0:06:32 - Epoch[0] - Iter: [3750/20019]	Time 0.095 (0.102)	Data 0.000 (0.007)	Loss 1.9501 (2.4590)	Prec 59.375 (50.305)	LR 0.3
INFO - 12/15/22 11:04:09 - 0:06:36 - Epoch[0] - Iter: [3800/20019]	Time 0.093 (0.102)	Data 0.000 (0.007)	Loss 1.8516 (2.4512)	Prec 59.375 (50.403)	LR 0.3
INFO - 12/15/22 11:04:14 - 0:06:41 - Epoch[0] - Iter: [3850/20019]	Time 0.095 (0.102)	Data 0.000 (0.007)	Loss 2.0076 (2.4436)	Prec 57.812 (50.506)	LR 0.3
INFO - 12/15/22 11:04:19 - 0:06:46 - Epoch[0] - Iter: [3900/20019]	Time 0.094 (0.101)	Data 0.000 (0.006)	Loss 1.7217 (2.4359)	Prec 64.062 (50.619)	LR 0.3
INFO - 12/15/22 11:04:23 - 0:06:51 - Epoch[0] - Iter: [3950/20019]	Time 0.095 (0.101)	Data 0.000 (0.006)	Loss 1.4350 (2.4277)	Prec 65.625 (50.734)	LR 0.3
INFO - 12/15/22 11:04:28 - 0:06:55 - Epoch[0] - Iter: [4000/20019]	Time 0.093 (0.101)	Data 0.000 (0.006)	Loss 1.7116 (2.4202)	Prec 57.812 (50.842)	LR 0.3
INFO - 12/15/22 11:04:33 - 0:07:00 - Epoch[0] - Iter: [4050/20019]	Time 0.096 (0.101)	Data 0.000 (0.006)	Loss 1.5074 (2.4133)	Prec 67.188 (50.941)	LR 0.3
INFO - 12/15/22 11:04:38 - 0:07:05 - Epoch[0] - Iter: [4100/20019]	Time 0.093 (0.101)	Data 0.000 (0.006)	Loss 2.3533 (2.4055)	Prec 53.125 (51.043)	LR 0.3
INFO - 12/15/22 11:04:42 - 0:07:10 - Epoch[0] - Iter: [4150/20019]	Time 0.095 (0.101)	Data 0.000 (0.006)	Loss 2.2127 (2.3989)	Prec 54.688 (51.135)	LR 0.3
INFO - 12/15/22 11:04:47 - 0:07:14 - Epoch[0] - Iter: [4200/20019]	Time 0.095 (0.101)	Data 0.000 (0.006)	Loss 1.8765 (2.3917)	Prec 54.688 (51.221)	LR 0.3
INFO - 12/15/22 11:04:52 - 0:07:19 - Epoch[0] - Iter: [4250/20019]	Time 0.095 (0.101)	Data 0.000 (0.006)	Loss 1.9098 (2.3846)	Prec 57.812 (51.322)	LR 0.3
INFO - 12/15/22 11:04:57 - 0:07:24 - Epoch[0] - Iter: [4300/20019]	Time 0.096 (0.101)	Data 0.000 (0.006)	Loss 1.5629 (2.3764)	Prec 68.750 (51.439)	LR 0.3
INFO - 12/15/22 11:05:01 - 0:07:29 - Epoch[0] - Iter: [4350/20019]	Time 0.094 (0.101)	Data 0.000 (0.006)	Loss 1.6852 (2.3699)	Prec 56.250 (51.531)	LR 0.3
INFO - 12/15/22 11:05:06 - 0:07:33 - Epoch[0] - Iter: [4400/20019]	Time 0.093 (0.101)	Data 0.000 (0.006)	Loss 1.0435 (2.3630)	Prec 71.875 (51.629)	LR 0.3
INFO - 12/15/22 11:05:11 - 0:07:38 - Epoch[0] - Iter: [4450/20019]	Time 0.095 (0.101)	Data 0.000 (0.006)	Loss 2.2865 (2.3563)	Prec 57.812 (51.732)	LR 0.3
INFO - 12/15/22 11:05:16 - 0:07:43 - Epoch[0] - Iter: [4500/20019]	Time 0.094 (0.101)	Data 0.000 (0.006)	Loss 1.3361 (2.3495)	Prec 60.938 (51.837)	LR 0.3
INFO - 12/15/22 11:05:20 - 0:07:48 - Epoch[0] - Iter: [4550/20019]	Time 0.094 (0.101)	Data 0.000 (0.006)	Loss 1.8586 (2.3439)	Prec 60.938 (51.911)	LR 0.3
INFO - 12/15/22 11:05:25 - 0:07:52 - Epoch[0] - Iter: [4600/20019]	Time 0.094 (0.100)	Data 0.000 (0.006)	Loss 1.9032 (2.3378)	Prec 54.688 (51.995)	LR 0.3
INFO - 12/15/22 11:05:30 - 0:07:57 - Epoch[0] - Iter: [4650/20019]	Time 0.094 (0.100)	Data 0.000 (0.005)	Loss 1.7826 (2.3317)	Prec 59.375 (52.074)	LR 0.3
INFO - 12/15/22 11:05:35 - 0:08:02 - Epoch[0] - Iter: [4700/20019]	Time 0.094 (0.100)	Data 0.000 (0.005)	Loss 1.6416 (2.3254)	Prec 59.375 (52.157)	LR 0.3
INFO - 12/15/22 11:05:39 - 0:08:07 - Epoch[0] - Iter: [4750/20019]	Time 0.095 (0.100)	Data 0.000 (0.005)	Loss 1.5045 (2.3192)	Prec 57.812 (52.243)	LR 0.3
INFO - 12/15/22 11:05:44 - 0:08:12 - Epoch[0] - Iter: [4800/20019]	Time 0.094 (0.100)	Data 0.000 (0.005)	Loss 1.7625 (2.3139)	Prec 60.938 (52.325)	LR 0.3
INFO - 12/15/22 11:05:49 - 0:08:16 - Epoch[0] - Iter: [4850/20019]	Time 0.104 (0.100)	Data 0.000 (0.005)	Loss 1.4130 (2.3084)	Prec 68.750 (52.402)	LR 0.3
INFO - 12/15/22 11:05:54 - 0:08:21 - Epoch[0] - Iter: [4900/20019]	Time 0.095 (0.100)	Data 0.000 (0.005)	Loss 1.8705 (2.3037)	Prec 60.938 (52.465)	LR 0.3
INFO - 12/15/22 11:05:59 - 0:08:26 - Epoch[0] - Iter: [4950/20019]	Time 0.094 (0.100)	Data 0.000 (0.005)	Loss 1.8821 (2.2982)	Prec 60.938 (52.537)	LR 0.3
INFO - 12/15/22 11:06:03 - 0:08:31 - Epoch[0] - Iter: [5000/20019]	Time 0.094 (0.100)	Data 0.000 (0.005)	Loss 2.0536 (2.2929)	Prec 45.312 (52.613)	LR 0.3
INFO - 12/15/22 11:06:08 - 0:08:35 - Epoch[0] - Iter: [5050/20019]	Time 0.094 (0.100)	Data 0.000 (0.005)	Loss 1.4472 (2.2874)	Prec 60.938 (52.693)	LR 0.3
INFO - 12/15/22 11:06:13 - 0:08:40 - Epoch[0] - Iter: [5100/20019]	Time 0.095 (0.100)	Data 0.000 (0.005)	Loss 2.2032 (2.2823)	Prec 51.562 (52.763)	LR 0.3
INFO - 12/15/22 11:06:18 - 0:08:45 - Epoch[0] - Iter: [5150/20019]	Time 0.096 (0.100)	Data 0.000 (0.005)	Loss 1.9541 (2.2771)	Prec 53.125 (52.840)	LR 0.3
INFO - 12/15/22 11:06:22 - 0:08:50 - Epoch[0] - Iter: [5200/20019]	Time 0.094 (0.100)	Data 0.000 (0.005)	Loss 1.9487 (2.2721)	Prec 56.250 (52.911)	LR 0.3
INFO - 12/15/22 11:06:27 - 0:08:54 - Epoch[0] - Iter: [5250/20019]	Time 0.097 (0.100)	Data 0.000 (0.005)	Loss 1.5748 (2.2673)	Prec 59.375 (52.984)	LR 0.3
INFO - 12/15/22 11:06:32 - 0:08:59 - Epoch[0] - Iter: [5300/20019]	Time 0.097 (0.100)	Data 0.000 (0.005)	Loss 1.8790 (2.2623)	Prec 57.812 (53.057)	LR 0.3
INFO - 12/15/22 11:06:37 - 0:09:04 - Epoch[0] - Iter: [5350/20019]	Time 0.094 (0.100)	Data 0.000 (0.005)	Loss 1.9208 (2.2578)	Prec 56.250 (53.121)	LR 0.3
INFO - 12/15/22 11:06:41 - 0:09:09 - Epoch[0] - Iter: [5400/20019]	Time 0.095 (0.100)	Data 0.000 (0.005)	Loss 1.9718 (2.2528)	Prec 54.688 (53.192)	LR 0.3
INFO - 12/15/22 11:06:46 - 0:09:13 - Epoch[0] - Iter: [5450/20019]	Time 0.094 (0.100)	Data 0.000 (0.005)	Loss 1.8551 (2.2474)	Prec 60.938 (53.276)	LR 0.3
INFO - 12/15/22 11:06:51 - 0:09:18 - Epoch[0] - Iter: [5500/20019]	Time 0.094 (0.100)	Data 0.000 (0.005)	Loss 1.5806 (2.2430)	Prec 60.938 (53.329)	LR 0.3
INFO - 12/15/22 11:06:56 - 0:09:23 - Epoch[0] - Iter: [5550/20019]	Time 0.094 (0.100)	Data 0.000 (0.005)	Loss 2.0381 (2.2384)	Prec 53.125 (53.395)	LR 0.3
INFO - 12/15/22 11:07:01 - 0:09:28 - Epoch[0] - Iter: [5600/20019]	Time 0.095 (0.100)	Data 0.000 (0.005)	Loss 1.0384 (2.2334)	Prec 65.625 (53.467)	LR 0.3
INFO - 12/15/22 11:07:05 - 0:09:33 - Epoch[0] - Iter: [5650/20019]	Time 0.094 (0.099)	Data 0.000 (0.005)	Loss 1.6970 (2.2288)	Prec 54.688 (53.528)	LR 0.3
INFO - 12/15/22 11:07:10 - 0:09:37 - Epoch[0] - Iter: [5700/20019]	Time 0.095 (0.099)	Data 0.000 (0.005)	Loss 1.7745 (2.2239)	Prec 57.812 (53.587)	LR 0.3
INFO - 12/15/22 11:07:15 - 0:09:42 - Epoch[0] - Iter: [5750/20019]	Time 0.095 (0.099)	Data 0.000 (0.004)	Loss 1.4720 (2.2201)	Prec 64.062 (53.641)	LR 0.3
INFO - 12/15/22 11:07:20 - 0:09:47 - Epoch[0] - Iter: [5800/20019]	Time 0.095 (0.099)	Data 0.000 (0.004)	Loss 1.8027 (2.2158)	Prec 62.500 (53.700)	LR 0.3
INFO - 12/15/22 11:07:24 - 0:09:52 - Epoch[0] - Iter: [5850/20019]	Time 0.095 (0.099)	Data 0.000 (0.004)	Loss 1.4433 (2.2118)	Prec 65.625 (53.756)	LR 0.3
